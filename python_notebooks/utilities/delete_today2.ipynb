{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a58de1-faae-4b1b-8496-ba38274ecc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Import all necessary modules ===\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, to_timestamp, expr, lit \n",
    "import boto3\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# === Start Spark session ===\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"twitch_emotes_analytics_project\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# === User input ===\n",
    "channel_name = input(\"Please Enter the Channel Name: \")\n",
    "chat_years_input = input(\"Please Enter the Chat Years (comma-separated): \")\n",
    "chat_years = [year.strip() for year in chat_years_input.split(\",\") if year.strip().isdigit() and len(year.strip()) == 4]\n",
    "print(f\"Processing years: {chat_years}\")\n",
    "\n",
    "# === S3 setup ===\n",
    "bucket = \"twitch-emotes-analytics-project\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# === Local path setup ===\n",
    "local_path = \"/Users/hari14/Desktop/PHD/twitch_emotes_analytics_project/data/processed_silver/\"\n",
    "\n",
    "# === Organize file ordering ===\n",
    "def order_files(json_files):\n",
    "    try:\n",
    "        return sorted(\n",
    "            json_files,\n",
    "            key=lambda x: int(os.path.basename(x).replace(\".json\", \"\").split(\"_\")[-2])\n",
    "        )\n",
    "    except (IndexError, ValueError) as e:\n",
    "        raise ValueError(f\"Filename parsing failed: {e}\")\n",
    "\n",
    "# === Initialize trackers for report ===\n",
    "total_files_per_year = {}\n",
    "processed_files_per_year = {}\n",
    "no_emote_files_per_year = {}\n",
    "missing_users_files_per_year = {}\n",
    "missing_users_count_per_year = {}\n",
    "\n",
    "# === Main processing loop per year ===\n",
    "for chat_year in chat_years:\n",
    "    processed_files = []\n",
    "    no_emote_files = []\n",
    "    missing_users_files = []\n",
    "\n",
    "    s3_key_prefix = f\"data/raw_bronze/{channel_name}/{chat_year}/\"\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=s3_key_prefix)\n",
    "    contents = response.get(\"Contents\", [])\n",
    "    \n",
    "    if not contents:\n",
    "        print(f\"üö´ No files found at S3 path: {s3_key_prefix}\")\n",
    "        continue\n",
    "\n",
    "    json_files = [i[\"Key\"] for i in contents]\n",
    "    sorted_json_files = order_files(json_files)\n",
    "    total_files_per_year[chat_year] = len(sorted_json_files)\n",
    "\n",
    "    for i, key in enumerate(sorted_json_files):\n",
    "        file_name = os.path.basename(key)\n",
    "        local_file_path = os.path.join(local_path, file_name)\n",
    "        temp_processed_path = os.path.join(local_path, \"temp_processed\")\n",
    "        file_vod_id = file_name.replace(\".json\", \"\").split(\"_\")[-1]\n",
    "\n",
    "        try:\n",
    "            # Download file\n",
    "            s3.download_file(Bucket=bucket, Key=key, Filename=local_file_path)\n",
    "            print(f\"üì• Downloading file {i+1}/{len(sorted_json_files)}: {file_name}\")\n",
    "\n",
    "            # Read JSON\n",
    "            df = spark.read.format(\"json\").option(\"multiLine\", True).load(local_file_path)\n",
    "            df_filtered = df.filter(col(\"emotes\").isNotNull())\n",
    "\n",
    "            if df_filtered.count() == 0:\n",
    "                no_emote_files.append(file_vod_id)\n",
    "                continue\n",
    "\n",
    "            df_exploded = df_filtered.withColumn(\"emote\", explode(\"emotes\"))\n",
    "\n",
    "            # Check for missing user info\n",
    "            missing_users_df = df_exploded.filter(col(\"author.id\").isNull() | col(\"author.name\").isNull())\n",
    "            missing_count = missing_users_df.count()\n",
    "            if missing_count > 0:\n",
    "                missing_users_files.append(file_vod_id)\n",
    "\n",
    "            # Add badge info\n",
    "            df_badges = df_exploded.withColumn(\"i_badge_names\", expr(\"transform(author.badges, x -> x.name)\")) \\\n",
    "                                   .withColumn(\"i_badge_titles\", expr(\"transform(author.badges, x -> x.title)\")) \\\n",
    "                                   .withColumn(\"i_badge_versions\", expr(\"transform(author.badges, x -> x.version)\"))\n",
    "\n",
    "            # Final selected columns\n",
    "            df_panel = df_badges.select(\n",
    "                col(\"author.id\").cast(\"long\").alias(\"i_user_id\"),\n",
    "                col(\"author.name\").alias(\"i_user_name\"),\n",
    "                col(\"author.colour\").alias(\"i_display_color\"),\n",
    "                col(\"i_badge_names\"),\n",
    "                col(\"i_badge_titles\"),\n",
    "                col(\"i_badge_versions\").cast(\"array<string>\"),\n",
    "                lit(None).cast(\"string\").alias(\"i_user_status\"),\n",
    "                lit(channel_name).alias(\"j_streamer\"),\n",
    "                col(\"emote.name\").alias(\"k_emote_name\"),\n",
    "                to_timestamp((col(\"timestamp\") / 1000000).cast(\"double\")).alias(\"t_timestamp\"),\n",
    "                col(\"time_text\").alias(\"t_time_text\"),\n",
    "                col(\"time_in_seconds\").cast(\"long\").alias(\"t_seconds\"),\n",
    "                lit(file_vod_id).alias(\"vod_id\"),\n",
    "                col(\"message\").alias(\"chat_message\")\n",
    "            )\n",
    "\n",
    "            # Write and upload parquet\n",
    "            df_panel.coalesce(1).write.mode(\"overwrite\").parquet(temp_processed_path)\n",
    "            parquet_files = glob.glob(os.path.join(temp_processed_path, \"part-*.parquet\"))\n",
    "            if not parquet_files:\n",
    "                raise FileNotFoundError(\"No parquet file found in temp_processed folder\")\n",
    "\n",
    "            parquet_file_path = parquet_files[0]\n",
    "            s3_output_key = f\"data/processed_silver/{channel_name}/{chat_year}/{file_name.replace('.json', '.parquet')}\"\n",
    "            s3.upload_file(parquet_file_path, bucket, s3_output_key)\n",
    "            print(f\"‚úÖ Uploaded: s3://{bucket}/{s3_output_key}\")\n",
    "            processed_files.append(file_vod_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing file {file_name}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            if os.path.exists(temp_processed_path):\n",
    "                shutil.rmtree(temp_processed_path)\n",
    "            if os.path.exists(local_file_path):\n",
    "                os.remove(local_file_path)\n",
    "            print(f\"üßπ Cleaned up local files for {file_name}\")\n",
    "\n",
    "    # Save year-level data\n",
    "    processed_files_per_year[chat_year] = processed_files\n",
    "    no_emote_files_per_year[chat_year] = no_emote_files\n",
    "    missing_users_files_per_year[chat_year] = missing_users_files\n",
    "    missing_users_count_per_year[chat_year] = len(missing_users_files)\n",
    "\n",
    "# === Generate Excel Summary ===\n",
    "summary_rows = []\n",
    "\n",
    "for year in chat_years:\n",
    "    summary_rows.append({\n",
    "        \"Year\": year,\n",
    "        \"Total S3 Files\": total_files_per_year.get(year, 0),\n",
    "        \"Processed Files\": len(processed_files_per_year.get(year, [])),\n",
    "        \"No Emote Files\": len(no_emote_files_per_year.get(year, [])),\n",
    "        \"Missing User Files\": missing_users_count_per_year.get(year, 0)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "excel_path = f\"twitch_emote_processing_summary_{channel_name}.xlsx\"\n",
    "summary_df.to_excel(excel_path, index=False)\n",
    "\n",
    "print(f\"\\nüìä Summary Excel report saved to: {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa25926-b281-4483-a077-b79d012dbd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(2013,2026))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645341a4-6423-4c80-8f9a-e7860be4da1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
